# Lab Report: CORPUS

#### David Polansky

## Process Description

> You should begin with _1-2 paragraphs_ outlining precisely what you did in the lab activity. While much in these paragraphs will repeat between students, it is worthwhile to write out your experience, both to help you remember what you did and because your experiences—and what you take away or emphasize—will differ in subtle but important ways from your classmates. Most of our experiential activities will include specific outcomes. You might produce a specific material product, such as a letterpress printed sheet, or be asked to adapt code we went over together to answer a new question. The first task of any lab report, then, will be to demonstrate completion of these tasks.

First, we imported a sci-fi corpus of 150 books from Project Gutenberg. Next, we matched words from that corpus with their associated sentiments using the NRC Word-Emotion Association Lexicon, which was compiled manually through MTurk crowdsourcing. We chose the nine books most heavily associated with one of these sentiments, then figured out which other sentiments these books draw from. Then, we graphed the positive or negative valence of these books over time. Finally, we used topic modelling to create 10 topics from these books (first, splitting the corpus into 100-word chunks, then creating a Document Term Matrix, then running the LDA function for k=10 buckets). We looked at the words most commonly associated with each topic (beta), and saw that we could investigate which topics the chunks draw from (gamma). (We kind of skimmed over this in class, and it wouldn't actually run when I tried it at home.)

## Observations

> In the next section of each report, you should in **2-3 paragraphs** move from a literal description of what you did in the lab to a more conceptual set of observations. In brief, you want to home in on those aspects of the lab that raised questions or prompted new insights into the textual technology we investigated in the lab activity. What new ideas occurred to you while working? What surprised, delighted, or frustrated you?

I was wondering how topic modelling goes from a DTM to a list of topics. In-lab, I misphrased the question as "how large is each bucket?" (the answer, of course, being "each bucket contains every word"), but what I meant was something more like, "if you could match each chunk to a bucket, would each bucket have about the same number of chunks?". After more investigation, I *think* the answer to that is no - which fits with the existence of a miscellaneous topic as well as the nine topics corresponding roughly to each book. The real answer to how topic modelling works definitely involves a lot of math, which I'm not quite as interested in.

Topic modelling and sentiment analysis are actually really similar, except sentiment analysis involves creating categories manually rather than algorithmically. It was interesting to see how clear or unclear the categories were: 'angry' was pretty obvious, but 'trust' and 'disgust' could have referred to either the subject or the object of the sentiment (e.g. the thing doing the trusting, or the thing being trusted?). And the contents of the sentiment library usually checked out, but words like 'abacus' did not: in what way is 'abacus' associated with trust? Why is 'abcess' assocated with sadness but not disgust? The dataset also has the limitation of not being from the time period the books were written in - to some extent, they used different words to refer to different sentiments.

## Analysis

> The final section of each report should bring your work in the lab into direct, critical conversation with our readings. In **_3-4 paragraphs_**, you should connect your lab observations to ideas from readings assigned _in the given lab unit_. This prose need not be as formal as a research paper, but it should demonstrate careful thought and preparation. You should integrate the readings explicitly, if possible through direct quotation. Use this writing to experiment with intellectual pairings you think might be generative to your larger thinking and help you prepare for the class’ Unessay projects. Think of your lab reports as an evolving research paper, and take them as seriously as one.

The Fugitive Verses project didn't rely on topic modelling, but instead on searching for commonly reprinted ngrams. This makes sense for its purpose: it's easier to find reprinted poetry by searching specifically for reprinted ngrams, than by searching for common topics (since a topic similar to "winter-themed writing" may include poems other than "Beautiful Snow", possibly in addition to weather-related articles). What would happen if you applied topic modelling *to* the reprinted poems? Would any unexpected patterns emerge? I'd be especially interested in how they differ from the topics present in present-day poetry.

But many of the differences that may be present, such as common metres and levels of vocabulary, wouldn't be identifiable with topic modelling. The more I think about it, the more unclear the actual applications of topic modelling become. Asking a question such as "what is this corpus about?" can also be answered by methods similar to the ones we used in the previous lab. You could use topic modelling to plot out an average book's transitions from topic to topic over time, but this can also be accomplished by reading a few books from that corpus (or using something simpler, like sentiment analysis). You could use topic modelling to organize a previously unorganized text into different topics, but I don't know if the resulting organization would make intuitive sense. One of the big drawbacks to this tool (at least, the way we used it in class) is that the only choices we have are the chunk size and the number of buckets. We can't select the topic we want to know about, because the model selects the topics for us. (Note: I'm sure there *are* great uses for topic modelling that I haven't thought of, and it's only from my perspective that it's a hammer in search of a nail.)

Topic modelling is inherently a form of categorization, and something similar can be applied to the gender classification that's discussed in "What Gets Counted Counts". Theoretically, an algorithm could "topic model" humans into n genders, then tell us which genders a particular human fits into the best. The issue with this approach is that we would have to choose the number of genders beforehand. (*Is* there a version of topic modelling that allows the algorithm to choose the number of buckets?) It would also have the issue of being imposed from the top down, depriving people of the right to self-identification. Ideally, the current "system" of gender identification would represent a distributed topic modelling of gender, but to me, it seems like humans are very flawed topic modellers working off of similarly-flawed datasets.