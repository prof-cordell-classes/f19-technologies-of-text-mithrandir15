# Lab Report: ALGORITHM

#### David Polansky

## Process Description

> You should begin with _1-2 paragraphs_ outlining precisely what you did in the lab activity. While much in these paragraphs will repeat between students, it is worthwhile to write out your experience, both to help you remember what you did and because your experiences—and what you take away or emphasize—will differ in subtle but important ways from your classmates. Most of our experiential activities will include specific outcomes. You might produce a specific material product, such as a letterpress printed sheet, or be asked to adapt code we went over together to answer a new question. The first task of any lab report, then, will be to demonstrate completion of these tasks.

First, I figured out how to open R Studio, and then how to open the source code. (The second part took longer than expected.) I was already familiar with variables, functions, and so forth, so I went through the document on my own, slightly ahead of the presentation up until the code block on line 126. There, I tried to figure out what ```%>%``` meant (and also went back to figure out the difference between ```barton$text``` and ```barton['text']```, and why parameters had to be specified in a particular function). From there, I ran the code at the pace of the presentation, and completed the lab at home. The code was essentially a collection of queries and visualizations of word frequency, mostly in Austen works pulled from Project Gutenberg with the gutenbergr library. 

## Observations

> In the next section of each report, you should in **2-3 paragraphs** move from a literal description of what you did in the lab to a more conceptual set of observations. In brief, you want to home in on those aspects of the lab that raised questions or prompted new insights into the textual technology we investigated in the lab activity. What new ideas occurred to you while working? What surprised, delighted, or frustrated you?

Initially, the presence of ```print("Hello, World!)``` led me to expect R to be a Python-like language. As it turns out, R *does* share some characteristics with Python (dynamic typing, no semicolons, array syntax), you can query it sort of like SQL (joins, grouping, filter). As mentioned before, I was trying to figure out the difference between ```barton$text``` and ```barton['text']``` - intuitively, they should do the same thing, but they have different outputs. (It turns out that ```barton$text``` just returns the data within the cell (assuming a 1x1 table), while ```barton['text']``` doesn't dereference the column (instead, in this case, it returns metadata for the column and a snippet of the actual data). This is a classic case of code clashing with intuition, but I hadn't been expecting this particular error. Similarly, I wasn't expecting `%>%` to be a pipe, both because I'm used to the symbol `|` representing pipe, and because *the pipes are backwards in R* compared to the Bourne shell. Also similarly, I tried to run `barton <- gutenberg_download(17780, meta_fields = c("title", "author"))` as `barton <- gutenberg_download(17780, c("title", "author"))` and was weirded out when it didn't work, because I'm familiar with languages that don't require you to specify the names of the parameters you're passing in (since they're distinguished by order). In all these cases, it turns out that some of the choices made by a programming language aren't necessary to make - as with normal languages. 

It looks incredibly easy to code a data visualization in R. (It also looks prettier and more versatile than Google Sheets, which has been my go-to graphing software.) This made me think that the other logic - comparing frequencies of words and ngrams - was chosen because of its ease of use. When analyzing a single text, this is certainly some of the easiest data to analyze programmatically, but it's probably not the most useful - just like one would search under a bright streetlamp even if the object you're looking for is probably in the dark.

## Analysis

> The final section of each report should bring your work in the lab into direct, critical conversation with our readings. In **_3-4 paragraphs_**, you should connect your lab observations to ideas from readings assigned _in the given lab unit_. This prose need not be as formal as a research paper, but it should demonstrate careful thought and preparation. You should integrate the readings explicitly, if possible through direct quotation. Use this writing to experiment with intellectual pairings you think might be generative to your larger thinking and help you prepare for the class’ Unessay projects. Think of your lab reports as an evolving research paper, and take them as seriously as one.

One of the things I found interesting about *Lovelace and Babbage* that didn't make it into my reading prep was that, even in the Pocket Universe, their machines never accomplished their intended purposes. In Lovelace & Babbage vs. the Client, Queen Victoria only accepts the immediate usefulness of auto-generated cat pictures. In Lovelace & Babbage vs. the Economic Model, their financial model (instead of working as intended) destroyed the Bank of London. In User Experience, the Analytical Engine does a destructive read of Carlyle's manuscript and inadvertently destroys the data *from* that destructive read. This could be a metaphor for how computational analysis can answer questions that you didn't ask, but not the one you did - as with the source code, where we found out that "the" was the most common word in Austen's corpus, and after the common words, names were the most common.

On Page 62, Queen Victoria asks, "If you put the wrong numbers in, does the right answer come out?" Babbage says this question is absurd and confused, but I don't think so: the "right" answer is, more precisely, the answer to the question you were trying to ask. If you put the wrong numbers in, the right answer (almost) never comes out. But the right answer to the wrong question may come out, which is (again) what we saw in the lab. (On a side note, Queen Victoria's "We look forward to advancements in the precognitive area" is also somewhat non-absurd, referencing Babbage's anticipatory carry (as well as caching algorithms that try to predict which piece of data you'll access next).)

Lovelace's program (confoundingly!) isn't in the book, but [I looked it up](https://upload.wikimedia.org/wikipedia/commons/c/cf/Diagram_for_the_computation_of_Bernoulli_numbers.jpg), and it wasn't quite what I expected. It featured barely any of the syntactical features of languages I'm familiar with - in fact, it looked more like mathematical notation. I shouldn't be surprised that it's syntactically different - after all, it was made for a very different computer architecture in a very different era. Unlike `%>%`, it's easily understandable by someone who knows algebra, and modern languages / pseudocode *could* use this format - again, the choices made by modern languages weren't necessary to make. At the same time, it's a bit harder to understand as a whole, especially compared to [the C code it translates to](https://gist.github.com/sinclairtarget/ad18ac65d277e453da5f479d6ccfc20e).